{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use TF IDF Vectoriser to transform text into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bibrec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m linear_kernel\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbibrec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserver\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mUtils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      8\u001b[0m books \u001b[39m=\u001b[39m get_books(\u001b[39m\"\u001b[39m\u001b[39m../data/BX-Books.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m book_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../data/editions_dump.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bibrec'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "from bibrec.server.Utils import *\n",
    "\n",
    "\n",
    "\n",
    "books = get_books(\"../data/BX-Books.csv\")\n",
    "book_data = pd.read_csv(\"../data/editions_dump.csv\")\n",
    "CHUNK_QUANTITY = 50\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "book_data = pd.DataFrame(book_data).drop_duplicates(subset=[\"isbn_10\"], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data to remove unnessecary spaces and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_data_preprocessing(self):\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].astype(\"string\")\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.lower())\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.replace(\"(\", ''))\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.replace(\")\", ''))\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.replace(',', ''))\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.replace('.', ''))\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.replace(\"'\", ''))\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.replace(\"--\", ''))\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: x.replace(\"  \", ' '))\n",
    "    book_data[\"book_info\"] = book_data[\"book_info\"].apply(lambda x: ' '.join(list(set(x.split()))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Similarity Matrix for all books using cosine similarity\n",
    "\n",
    "In order to avoid running out of memory while calculating the Matrix we decided to split the data into smaller chunks\n",
    "- This has the effect that only similarities for books in the same chunk are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(self):\n",
    "    # data cleaning\n",
    "    self.book_data.rename(columns={'isbn_10': 'isbn', 'first_sentence.value': 'first_sentence', 'notes.value': 'notes'}, inplace=True)\n",
    "    self.book_data.isbn = self.book_data.isbn.map(lambda x: str(x).strip()[2:-2])\n",
    "    self.book_data = self.books.merge(self.book_data, on=\"isbn\", how=\"inner\")\n",
    "    self.book_data = self.book_data.drop(columns=[\"Unnamed: 0\", \"isbn_13\", \"publishers\", \"title\", \"first_sentence.type\", \"notes.type\"])\n",
    "    self.book_data.subjects = self.book_data.subjects.map(lambda x: str(x).strip()[2:-2])\n",
    "    self.book_data.genres = self.book_data.genres.map(lambda x: str(x).strip()[2:-2])\n",
    "    self.book_data = self.book_data.fillna(\" \")\n",
    "\n",
    "    # add book_info column and append relevant data\n",
    "    self.book_data[\"book_info\"] = self.book_data[\"book_title\"] + \" \" + self.book_data[\"subjects\"] + \" \" + self.book_data[\"genres\"]\n",
    "    self.book_data_preprocessing()\n",
    "\n",
    "    self.similarities = []\n",
    "    self.mappings = []\n",
    "    for i in range(0, self.CHUNK_QUANTITY):\n",
    "        len_books = self.book_data.shape[0] / self.CHUNK_QUANTITY\n",
    "        chunk_data = self.book_data[int(i * len_books):int((i + 1) * len_books)]\n",
    "        chunk_data = chunk_data.reset_index(drop=True)\n",
    "        similarity_matrix = self.get_similarity_matrix(chunk_data)\n",
    "        self.similarities.append(similarity_matrix)\n",
    "        self.mappings.append(pd.Series(chunk_data.index, index=chunk_data[\"isbn13\"]))\n",
    "\n",
    "def get_similarity_matrix(self, book_data):\n",
    "    tfidf_matrix = self.tfidf.fit_transform(book_data[\"book_info\"])\n",
    "    similarity_matrix = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All similar items are gathered for the given isbn and sorted in a descending order.\n",
    "The number of returned items is split at n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_tf_idf(self, isbn13, n=15):\n",
    "    book_indices = []\n",
    "    for i in range(0, self.CHUNK_QUANTITY):\n",
    "        if isbn13 in self.mappings[i].index:\n",
    "            similarity_matrix = self.similarities[i]\n",
    "            book_index = self.mappings[i][isbn13]\n",
    "\n",
    "            if isinstance(book_index, pd.Series):\n",
    "                book_index = book_index[0]\n",
    "            similarity_score = list(enumerate(similarity_matrix[book_index]))\n",
    "            # Sort the books based on the similarity scores\n",
    "            similarity_score = sorted(similarity_score, key=lambda x: x[1], reverse=True)\n",
    "            similarity_score = similarity_score[1:n + 1]\n",
    "\n",
    "            # Check if all similarities are 0\n",
    "            if all(i[1] == 0 for i in similarity_score):\n",
    "                return None\n",
    "\n",
    "            book_indices.append([i[0] for i in similarity_score])\n",
    "\n",
    "    if len(book_indices) == 0:\n",
    "        return None\n",
    "    return self.book_data[[\"book_title\", \"isbn13\", \"isbn\"]].iloc[book_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_similarities()\n",
    "recommend_tf_idf(\"9780002005883\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9a40f058525f2d65f9906156bda6efa59e0d787a10d236efb1510ab56f2f48e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
